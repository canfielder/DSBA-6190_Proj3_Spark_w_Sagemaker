{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook developes a Spam message identification model using pyspark in an AWS Sagemaker evironment. The dataset for this model is the UCI SMS Spam Collection Data Set found [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). While this dataset is small, the problem this notebook addresses, Spam messages, is a problem that encompasses large enough amounts of data to be a task suited for the Spark platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages / Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import boto3\n",
    "import bokeh\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker_pyspark\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, \\\n",
    "                               HashingTF, IDF\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType,\\\n",
    "                              IntegerType, StringType\n",
    "\n",
    "from sagemaker_pyspark import RandomNamePolicyFactory, IAMRole,\\\n",
    "                              EndpointCreationPolicy, SageMakerModel,\\\n",
    "                              SageMakerResourceCleanup\n",
    "from sagemaker_pyspark.transformation.serializers \\\n",
    "     import ProtobufRequestRowSerializer\n",
    "from sagemaker_pyspark.transformation.serializers.serializers \\\n",
    "     import LibSVMRequestRowSerializer\n",
    "\n",
    "#Set Seed for Random Actions\n",
    "seed = 5590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AWS and Spark\n",
    "The following code blocks set up the global values and settings for AWS and Spark parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket = \"dsba-6190-project3-spark\"\n",
    "file_name = \"spam.csv\"\n",
    "session = sagemaker.Session()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-13-251.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spam Filter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f52bc0a0710>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "# See the SageMaker Spark Github to learn how to connect to EMR from a notebook instance\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Spam Filter\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "When we try to load the data using default settings, the import adds headers and includes several empty columns. To avoid this, we define the schema of the data before we import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Schema\n",
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      "\n",
      "\n",
      "Shape - Rows x Columns\n",
      "5574 x 2\n"
     ]
    }
   ],
   "source": [
    "# Define Known Schema\n",
    "schema = StructType([\n",
    "    StructField(\"class\", StringType()),\n",
    "    StructField(\"sms\", StringType())\n",
    "])\n",
    "\n",
    "# Import CSV\n",
    "df = spark.read\\\n",
    "          .schema(schema)\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .csv('s3a://{}/{}'.format(bucket, file_name))\n",
    "\n",
    "df_num_col =  len(df.columns)\n",
    "df_num_rows = df.count()\n",
    "\n",
    "# Inspect Import\n",
    "df.show(5)\n",
    "print()\n",
    "print(\"Schema\")\n",
    "df.printSchema()\n",
    "print()\n",
    "print(\"Shape - Rows x Columns\")\n",
    "print(df_num_rows,\"x\", df_num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Null Values\n",
    "We need to check and see if our data contains any null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| class| sms|\n",
      "+------+----+\n",
      "|ham\"\"\"|null|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears one of the rows contains null values, and the class label is corrupted as well. We can go ahead and drop this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now inspect the dataframe now that the null row has been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Schema\n",
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      "\n",
      "\n",
      "Shape - Rows x Columns\n",
      "5573 x 2\n"
     ]
    }
   ],
   "source": [
    "df_num_col =  len(df.columns)\n",
    "df_num_rows = df.count()\n",
    "\n",
    "# Inspect Import\n",
    "df.show(5)\n",
    "print()\n",
    "print(\"Schema\")\n",
    "df.printSchema()\n",
    "print()\n",
    "print(\"Shape - Rows x Columns\")\n",
    "print(df_num_rows,\"x\", df_num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-check null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|class|sms|\n",
      "+-----+---+\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll look at the breakdown of our target variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| class|count|\n",
      "+------+-----+\n",
      "|ham\"\"\"|    1|\n",
      "|   ham| 4825|\n",
      "|  spam|  747|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is an error with the class label on one value. Lets see what the message is associated with **ham\"\"\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------+\n",
      "|class |sms                                                      |\n",
      "+------+---------------------------------------------------------+\n",
      "|ham\"\"\"|Well there's still a bit left if you guys want to tonight|\n",
      "+------+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(f.col(\"class\") == 'ham\"\"\"').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem out of the oridinary, and appears to be a **ham** sms message. I am going to change **ham\"\"\"** to **ham**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"class\", f.when(f.col(\"class\") == 'ham\"\"\"' , 'ham').\n",
    "                     otherwise(f.col(\"class\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets verify the change occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  ham| 4826|\n",
      "| spam|  747|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "Before we perform any analysis on the data we will need to perform two major steps:\n",
    "\n",
    "1. Text Normalization\n",
    "2. Tokenization \n",
    "\n",
    "With Text Normalization, we will process the raw text to provide a quality input for our model. These actions used the blog post [**Spam classification using Spark’s DataFrames, ML and Zeppelin (Part 1)**](https://blog.codecentric.de/en/2016/06/spam-classification-using-sparks-dataframes-ml-zeppelin-part-1/) by Daniel Pape, accessed on 4/16/2020, as guidance for some of these actions. This blog post provided a good framework particularly for handling types of text you find in an SMS message, such as emoticons.\n",
    "\n",
    "Once the raw text is normalized, we can then tokenize and convert the text into a form that can be used by the analytical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "To normalize the text, there are several steps we plan on taking:\n",
    "\n",
    "1. Convert all text to lowercase\n",
    "2. Convert all numbers to the text **_\" normalized_number \"_**\n",
    "3. Convert all emoticons to the text **_\" normalized_emoticon \"_**\n",
    "4. Convert all currency symbols to the text **_\" normalized_currency_symbol \"_**\n",
    "5. Convert all links to the text **_\" normalized_url \"_**\n",
    "6. Convert all email addresses to the text **_\" normalized_email \"_**\n",
    "7. Convert all diamond/question mark symbols to the text **_\" normalized_doamond_symbol \"_**\n",
    "8. Remove HTML characters\n",
    "9. Remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Text to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|\n",
      "+-----+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|ok lar... joking ...|\n",
      "| spam|Free entry in 2 a...|free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|u dun say so earl...|\n",
      "|  ham|Nah I don't think...|nah i don't think...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm = df.select(\"class\",\"sms\", f.lower(f.col(\"sms\")).alias(\"sms_norm\"))\n",
    "df_norm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Symbols and Objects\n",
    "To normalize the symbols and objects in the data, we will need to define user functions and employ replacement with regex tools.\n",
    "\n",
    "To enable a method to cycle through the dataframe and make all the necessary replacements, I am going to define a dictionary, where each key is the expression that will be used to find what needs to be replaces, and the value is the repalcement string.\n",
    "\n",
    "The regex for the emoticons came from [here](https://www.regextester.com/96995).\n",
    "\n",
    "The remaining regex expressions came from [here](https://github.com/daniel-pape/spark-logistic-regression-spam-sms/blob/master/src/main/scala/preprocessing/LineCleaner.scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = [\"&lt;\", \"&gt;\", \"&amp;\", \"&cent;\", \"&pound;\", \"&yen;\", \"&euro;\", \"&copy;\", \"&reg;\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_url = \"\\\\w+(\\\\.|-)*\\\\w+@.*\\\\.(com|de|uk)\"\n",
    "regex_emoticon = \":\\)|:-\\)|:\\(|:-\\(|;\\);-\\)|:-O|8-|:P|:D|:\\||:S|:\\$|:@|8o\\||\\+o\\(|\\(H\\)|\\(C\\)|\\(\\?\\)\"\n",
    "regex_number = \"\\\\d+\"\n",
    "regex_punctuation =\"[\\\\.\\\\,\\\\:\\\\-\\\\!\\\\?\\\\n\\\\t,\\\\%\\\\#\\\\*\\\\|\\\\=\\\\(\\\\)\\\\\\\"\\\\>\\\\<\\\\/]\"\n",
    "regex_currency = \"[\\\\$\\\\€\\\\£]\"\n",
    "regex_url =  \"(http://|https://)?www\\\\.\\\\w+?\\\\.(de|com|co.uk)\"\n",
    "regex_diamond_question = \"�\"\n",
    "regex_html = \"|\".join(html_list)\n",
    "\n",
    "dict_norm = {\n",
    "    regex_emoticon : \" normalized_emoticon \",\n",
    "    regex_url : \" normalized_emailadress \",\n",
    "    regex_number : \" normalized_number \",\n",
    "    regex_punctuation : \" \",\n",
    "    regex_currency : \" normalized_currency_symbol \",\n",
    "    regex_url: \" normalized_url \",\n",
    "    regex_diamond_question : \" normalized_doamond_symbol \",\n",
    "    regex_html : \" \"\n",
    "}\n",
    "\n",
    "#dict_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_norm.items():\n",
    "    df_norm = df_norm.withColumn(\"sms_norm\", f.regexp_replace(f.col(\"sms_norm\"),key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sms_norm                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|go until jurong point  crazy   available only in bugis n great world la e buffet    cine there got amore wat                                                                                                                                        |\n",
      "|ok lar    joking wif u oni                                                                                                                                                                                                                          |\n",
      "|free entry in  normalized_number  a wkly comp to win fa cup final tkts  normalized_number st may  normalized_number   text fa to  normalized_number  to receive entry question std txt rate t&c's apply  normalized_number over normalized_number 's|\n",
      "|u dun say so early hor    u c already then say                                                                                                                                                                                                      |\n",
      "|nah i don't think he goes to usf  he lives around here though                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm.select('sms_norm').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape - Rows x Columns\n",
      "5573 x 3\n"
     ]
    }
   ],
   "source": [
    "df = df_norm.dropna()\n",
    "\n",
    "print(\"Shape - Rows x Columns\")\n",
    "print(df.count(),\"x\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to check again for null values, to ensure the conversion hasn't created any new null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+\n",
      "|class|sms|sms_norm|\n",
      "+-----+---+--------+\n",
      "+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_norm\n",
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Class to Binary\n",
    "We need to convert our spam/ham class to a binary. We also need to conert the column type to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Go until jurong p...|go until jurong p...|\n",
      "|    0|Ok lar... Joking ...|ok lar    joking ...|\n",
      "|    1|Free entry in 2 a...|free entry in  no...|\n",
      "|    0|U dun say so earl...|u dun say so earl...|\n",
      "|    0|Nah I don't think...|nah i don't think...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm = df_norm.withColumn(\"class\", f.when(f.col(\"class\") == \"spam\" , 1).\n",
    "                             when(f.col(\"class\") == \"ham\" , 0).\n",
    "                             otherwise(f.col(\"class\")))\n",
    "\n",
    "df_norm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      " |-- sms_norm: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Go until jurong p...|go until jurong p...|\n",
      "|    0|Ok lar... Joking ...|ok lar    joking ...|\n",
      "|    1|Free entry in 2 a...|free entry in  no...|\n",
      "|    0|U dun say so earl...|u dun say so earl...|\n",
      "|    0|Nah I don't think...|nah i don't think...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- class: integer (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      " |-- sms_norm: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm = df_norm.withColumn(\"class\", f.col('class').cast(IntegerType()))\n",
    "df_norm.show(5)\n",
    "df_norm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization\n",
    "We will tokenize the text using a pyspark pipeline. First, we must initialize the pipeline components. For this pipeline, we will user the following estimators:\n",
    "\n",
    "1. Tokenizer\n",
    "2. Stop Words Remover\n",
    "3. Term Frequency Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sms_norm\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"features_tf\", numFeatures=1000)\n",
    "\n",
    "pipeline_text = Pipeline(stages=[tokenizer, remover, hashingTF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Pipeline on Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|              tokens|     tokens_filtered|         features_tf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|Go until jurong p...|go until jurong p...|[go, until, juron...|[go, jurong, poin...|(1000,[7,77,150,1...|\n",
      "|    0|Ok lar... Joking ...|ok lar    joking ...|[ok, lar, , , , j...|[ok, lar, , , , j...|(1000,[20,372,484...|\n",
      "|    1|Free entry in 2 a...|free entry in  no...|[free, entry, in,...|[free, entry, , n...|(1000,[35,73,128,...|\n",
      "|    0|U dun say so earl...|u dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(1000,[57,368,372...|\n",
      "|    0|Nah I don't think...|nah i don't think...|[nah, i, don't, t...|[nah, think, goes...|(1000,[135,210,32...|\n",
      "|    1|FreeMsg Hey there...|freemsg hey there...|[freemsg, hey, th...|[freemsg, hey, da...|(1000,[36,91,98,9...|\n",
      "|    0|Even my brother i...|even my brother i...|[even, my, brothe...|[even, brother, l...|(1000,[18,47,48,5...|\n",
      "|    0|As per your reque...|as per your reque...|[as, per, your, r...|[per, request, 'm...|(1000,[36,71,92,1...|\n",
      "|    1|WINNER!! As a val...|winner   as a val...|[winner, , , as, ...|[winner, , , valu...|(1000,[39,43,74,1...|\n",
      "|    1|Had your mobile 1...|had your mobile  ...|[had, your, mobil...|[mobile, , normal...|(1000,[36,73,82,1...|\n",
      "|    0|I'm gonna be home...|i'm gonna be home...|[i'm, gonna, be, ...|[gonna, home, soo...|(1000,[41,159,263...|\n",
      "|    1|SIX chances to wi...|six chances to wi...|[six, chances, to...|[six, chances, wi...|(1000,[15,35,92,2...|\n",
      "|    1|URGENT! You have ...|urgent  you have ...|[urgent, , you, h...|[urgent, , won, ,...|(1000,[68,73,122,...|\n",
      "|    0|I've been searchi...|i've been searchi...|[i've, been, sear...|[searching, right...|(1000,[36,39,140,...|\n",
      "|    0|I HAVE A DATE ON ...|i have a date on ...|[i, have, a, date...|      [date, sunday]|(1000,[44,82,170,...|\n",
      "|    1|XXXMobileMovieClu...|xxxmobilemovieclu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(1000,[43,66,135,...|\n",
      "|    0|Oh k...i'm watchi...|oh k   i'm watchi...|[oh, k, , , i'm, ...|[oh, k, , , watch...|(1000,[135,275,37...|\n",
      "|    0|Eh u remember how...|eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|(1000,[14,15,80,1...|\n",
      "|    0|Fine if that��s t...|fine if that norm...|[fine, if, that, ...|[fine, normalized...|(1000,[115,129,15...|\n",
      "|    1|England v Macedon...|england v macedon...|[england, v, mace...|[england, v, mace...|(1000,[4,9,19,71,...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_text_fit = pipeline_text.fit(df_norm)\n",
    "df_pipeline = pipeline_text_fit.transform(df_norm)\n",
    "df_pipeline.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split\n",
    "Now that we have performed all the possible actions that should be performed on the complete dataset, we split the data into train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train = 0.8\n",
    "train, test = df_pipeline.randomSplit([split_train, (1-split_train)], seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape - Train\n",
      "(4423, 6)\n",
      "\n",
      "Shape - Test\n",
      "(1150, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape - Train\")\n",
    "print((train.count(), len(train.columns)))\n",
    "print()\n",
    "print(\"Shape - Test\")\n",
    "print((test.count(), len(test.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency Calculation\n",
    "To calculate the Term Frequency - Inverse Document Frequency values for the corpus, we need to train the IDF estmator on the **train** data. Then we apply the trained estimator to the train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize IDF Estimator\n",
    "idf = IDF(minDocFreq=2, inputCol=\"features_tf\", outputCol=\"features_tfidf\")\n",
    "\n",
    "# Train IDF Estimator to Term Frequency Data\n",
    "idfModel = idf.fit(train)\n",
    "\n",
    "# Re-Scale Term Frequency Data to \n",
    "train = idfModel.transform(train)\n",
    "test = idfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|              tokens|     tokens_filtered|         features_tf|      features_tfidf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0| &lt;#&gt;  in mc...|      in mca  but...|[, , , , , , in, ...|[, , , , , , mca,...|(1000,[18,83,372,...|(1000,[18,83,372,...|\n",
      "|    0| &lt;DECIMAL&gt; ...|  decimal  m but ...|[, , decimal, , m...|[, , decimal, , m...|(1000,[18,62,83,1...|(1000,[18,62,83,1...|\n",
      "|    0| and  picking the...| and  picking the...|[, and, , picking...|[, , picking, var...|(1000,[128,263,33...|(1000,[128,263,33...|\n",
      "|    0| came to look at ...| came to look at ...|[, came, to, look...|[, came, look, fl...|(1000,[7,146,197,...|(1000,[7,146,197,...|\n",
      "|    0| gonna let me kno...| gonna let me kno...|[, gonna, let, me...|[, gonna, let, kn...|(1000,[51,134,164...|(1000,[51,134,164...|\n",
      "|    0| said kiss, kiss,...| said kiss  kiss ...|[, said, kiss, , ...|[, said, kiss, , ...|(1000,[44,133,168...|(1000,[44,133,168...|\n",
      "|    0| says that he's q...| says that he's q...|[, says, that, he...|[, says, quitting...|(1000,[122,138,17...|(1000,[122,138,17...|\n",
      "|    0| what number do u...| what number do u...|[, what, number, ...|[, number, u, liv...|(1000,[281,372,49...|(1000,[281,372,49...|\n",
      "|    0|\"7 wonders in My ...|  normalized_numb...|[, , normalized_n...|[, , normalized_n...|(1000,[19,104,150...|(1000,[19,104,150...|\n",
      "|    0|\"7 wonders in My ...|  normalized_numb...|[, , normalized_n...|[, , normalized_n...|(1000,[19,104,150...|(1000,[19,104,150...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|              tokens|     tokens_filtered|         features_tf|      features_tfidf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0| &lt;#&gt;  mins ...|      mins but i ...|[, , , , , , mins...|[, , , , , , mins...|(1000,[73,83,183,...|(1000,[73,83,183,...|\n",
      "|    0|\"7 wonders in My ...|  normalized_numb...|[, , normalized_n...|[, , normalized_n...|(1000,[19,104,150...|(1000,[19,104,150...|\n",
      "|    0|\"A Boy loved a ga...| a boy loved a ga...|[, a, boy, loved,...|[, boy, loved, ga...|(1000,[24,29,62,6...|(1000,[24,29,62,6...|\n",
      "|    0|\"Awesome question...| awesome question...|[, awesome, quest...|[, awesome, quest...|(1000,[19,122,159...|(1000,[19,122,159...|\n",
      "|    0|\"Cool breeze... B...| cool breeze    b...|[, cool, breeze, ...|[, cool, breeze, ...|(1000,[170,196,29...|(1000,[170,196,29...|\n",
      "|    0|\"Don't Think Abou...| don't think abou...|[, don't, think, ...|[, think, \\what, ...|(1000,[7,168,237,...|(1000,[7,168,237,...|\n",
      "|    0|\"Happy or sad , o...| happy or sad   o...|[, happy, or, sad...|[, happy, sad, , ...|(1000,[44,141,168...|(1000,[44,141,168...|\n",
      "|    0|\"Storming msg: We...| storming msg  we...|[, storming, msg,...|[, storming, msg,...|(1000,[9,15,29,44...|(1000,[9,15,29,44...|\n",
      "|    0|\"Today is \\song d...| today is \\song d...|[, today, is, \\so...|[, today, \\song, ...|(1000,[19,36,83,8...|(1000,[19,36,83,8...|\n",
      "|    0|\"U need my presnt...| u need my presnt...|[, u, need, my, p...|[, u, need, presn...|(1000,[7,11,13,26...|(1000,[7,11,13,26...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate Data\n",
    "Moving forward we only need the tf-idf features and the class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(1000,[18,83,372,...|\n",
      "|    0|(1000,[18,62,83,1...|\n",
      "|    0|(1000,[128,263,33...|\n",
      "|    0|(1000,[7,146,197,...|\n",
      "|    0|(1000,[51,134,164...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select Data\n",
    "df_train = train.select(f.col(\"class\").alias(\"label\"), f.col(\"features_tfidf\").alias(\"features\"))\n",
    "df_train.show(5)\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(1000,[73,83,183,...|\n",
      "|    0|(1000,[19,104,150...|\n",
      "|    0|(1000,[24,29,62,6...|\n",
      "|    0|(1000,[19,122,159...|\n",
      "|    0|(1000,[170,196,29...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = test.select(f.col(\"class\").alias(\"label\"), f.col(\"features_tfidf\").alias(\"features\"))\n",
    "df_test.show(5)\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(label,DoubleType,true),StructField(features,VectorUDT,true)))\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType()),\n",
    "    StructField(\"features\", VectorUDT())\n",
    "])\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/apache/spark/blob/930b90a84871e2504b57ed50efa7b8bb52d3ba44/mllib/src/main/scala/org/apache/spark/ml/source/libsvm/LibSVMRelation.scala#L79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "df_train = df_train.withColumn(\"label\", f.col('label').cast(DoubleType()))\n",
    "df_train = df_train.withColumn(\"features\", f.col('features').cast(VectorUDT()))\n",
    "\n",
    "# Test\n",
    "df_test = df_test.withColumn(\"label\", f.col('label').cast(DoubleType()))\n",
    "df_test = df_test.withColumn(\"features\", f.col('features').cast(VectorUDT()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_estimator = XGBoostSageMakerEstimator(\n",
    "  sagemakerRole = IAMRole(role),\n",
    "  requestRowSerializer = LibSVMRequestRowSerializer(featuresColumnName=\"features\",\n",
    "                                                    schema=schema,\n",
    "                                                    labelColumnName=\"label\"),\n",
    "  trainingInstanceType = \"ml.m4.xlarge\",\n",
    "  trainingInstanceCount = 1,\n",
    "  endpointInstanceType = \"ml.m4.xlarge\",\n",
    "  endpointInitialInstanceCount = 1,\n",
    "  namePolicyFactory=RandomNamePolicyFactory(\"spam-xgb-\"),\n",
    "  endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM\n",
    ")\n",
    "\n",
    "xgboost_estimator.setNumRound(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = xgboost_estimator.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, prediction: double]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans = xgboost_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------------+\n",
      "|label|            features|      prediction|\n",
      "+-----+--------------------+----------------+\n",
      "|  0.0|(1000,[73,83,183,...| 0.0366459488869|\n",
      "|  0.0|(1000,[19,104,150...| 0.0501010119915|\n",
      "|  0.0|(1000,[24,29,62,6...| 0.0635778605938|\n",
      "|  0.0|(1000,[19,122,159...|0.00289225578308|\n",
      "|  0.0|(1000,[170,196,29...|0.00289225578308|\n",
      "+-----+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_trans.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-282-b9f8dccb7919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute raw scores on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictionAndLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgboost_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Instantiate metrics object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionAndLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = df_test.map(lambda lp: (float(xgboost_model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a trained PCA estimator and K-Means model. we now need to evalute the pipeline performance with the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "First, we transform the test data to generate predicted clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedData = pipelineModelSM.transform(testData)\n",
    "transformedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display a digit\n",
    "def showDigit(img, caption='', xlabel='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axes.get_xaxis().set_ticks([])\n",
    "    subplot.axes.get_yaxis().set_ticks([])\n",
    "    plt.title(caption)\n",
    "    plt.xlabel(xlabel)\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    \n",
    "def displayClusters(data):\n",
    "    images = np.array(data.select(\"features\").cache().take(250))\n",
    "    clusters = data.select(\"closest_cluster\").cache().take(250)\n",
    "\n",
    "    for cluster in range(10):\n",
    "        print('\\n\\n\\nCluster {}:'.format(string.ascii_uppercase[cluster]))\n",
    "        digits = [ img for l, img in zip(clusters, images) if int(l.closest_cluster) == cluster ]\n",
    "        height=((len(digits)-1)//5)+1\n",
    "        width=5\n",
    "        plt.rcParams[\"figure.figsize\"] = (width,height)\n",
    "        _, subplots = plt.subplots(height, width)\n",
    "        subplots=np.ndarray.flatten(subplots)\n",
    "        for subplot, image in zip(subplots, digits):\n",
    "            showDigit(image, subplot=subplot)\n",
    "        for subplot in subplots[len(digits):]:\n",
    "            subplot.axis('off')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayClusters(transformedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-Up\n",
    "I do not want to be needlessly charged for deployed enpoints after this example is through, so I need to shut everything down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUp(model):\n",
    "    resource_cleanup = SageMakerResourceCleanup(model.sagemakerClient)\n",
    "    resource_cleanup.deleteResources(model.getCreatedResources())\n",
    "    \n",
    "# Delete the SageMakerModel in pipeline\n",
    "for m in pipelineModelSM.stages:\n",
    "    if isinstance(m, SageMakerModel):\n",
    "        cleanUp(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
