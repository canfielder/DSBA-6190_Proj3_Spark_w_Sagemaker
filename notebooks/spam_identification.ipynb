{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook developes a Spam message identification model using pyspark in an AWS Sagemaker evironment. The dataset for this model is the UCI SMS Spam Collection Data Set found [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). While this dataset is small, the problem this notebook addresses, Spam messages, is a problem that encompasses large enough amounts of data to be a task suited for the Spark platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages / Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import boto3\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker_pyspark\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, \\\n",
    "                               HashingTF, IDF\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType,\\\n",
    "                              IntegerType, StringType\n",
    "\n",
    "from sagemaker_pyspark import RandomNamePolicyFactory, IAMRole,\\\n",
    "                              EndpointCreationPolicy, SageMakerModel,\\\n",
    "                              SageMakerResourceCleanup\n",
    "from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator, PCASageMakerEstimator\n",
    "from sagemaker_pyspark.transformation.serializers \\\n",
    "     import ProtobufRequestRowSerializer\n",
    "from sagemaker_pyspark.transformation.serializers.serializers \\\n",
    "     import LibSVMRequestRowSerializer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Set Seed for Random Actions\n",
    "seed = 5590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AWS and Spark\n",
    "The following code blocks set up the global values and settings for AWS and Spark parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket = \"dsba-6190-project3-spark\"\n",
    "file_name = \"spam.csv\"\n",
    "session = sagemaker.Session()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-107-99.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spam Filter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fca27197390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "# See the SageMaker Spark Github to learn how to connect to EMR from a notebook instance\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Spam Filter\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "When we try to load the data using default settings, the import adds headers and includes several empty columns. To avoid this, we define the schema of the data before we import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Schema\n",
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      "\n",
      "\n",
      "Shape - Rows x Columns\n",
      "5574 x 2\n"
     ]
    }
   ],
   "source": [
    "# Define Known Schema\n",
    "schema = StructType([\n",
    "    StructField(\"class\", StringType()),\n",
    "    StructField(\"sms\", StringType())\n",
    "])\n",
    "\n",
    "# Import CSV\n",
    "df = spark.read\\\n",
    "          .schema(schema)\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .csv('s3a://{}/{}'.format(bucket, file_name))\n",
    "\n",
    "df_num_col =  len(df.columns)\n",
    "df_num_rows = df.count()\n",
    "\n",
    "# Inspect Import\n",
    "df.show(5)\n",
    "print()\n",
    "print(\"Schema\")\n",
    "df.printSchema()\n",
    "print()\n",
    "print(\"Shape - Rows x Columns\")\n",
    "print(df_num_rows,\"x\", df_num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Null Values\n",
    "We need to check and see if our data contains any null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| class| sms|\n",
      "+------+----+\n",
      "|ham\"\"\"|null|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears one of the rows contains null values, and the class label is corrupted as well. We can go ahead and drop this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now inspect the dataframe now that the null row has been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Schema\n",
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      "\n",
      "\n",
      "Shape - Rows x Columns\n",
      "5573 x 2\n"
     ]
    }
   ],
   "source": [
    "df_num_col =  len(df.columns)\n",
    "df_num_rows = df.count()\n",
    "\n",
    "# Inspect Import\n",
    "df.show(5)\n",
    "print()\n",
    "print(\"Schema\")\n",
    "df.printSchema()\n",
    "print()\n",
    "print(\"Shape - Rows x Columns\")\n",
    "print(df_num_rows,\"x\", df_num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-check null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|class|sms|\n",
      "+-----+---+\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "We'll do some very basic EDA here. First we'll look at the breakdown of our target variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| class|count|\n",
      "+------+-----+\n",
      "|ham\"\"\"|    1|\n",
      "|   ham| 4825|\n",
      "|  spam|  747|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is an error with the class label on one value. Lets see what the message is associated with **ham\"\"\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------+\n",
      "|class |sms                                                      |\n",
      "+------+---------------------------------------------------------+\n",
      "|ham\"\"\"|Well there's still a bit left if you guys want to tonight|\n",
      "+------+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(f.col(\"class\") == 'ham\"\"\"').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem out of the oridinary, and appears to be a **ham** sms message. I am going to change **ham\"\"\"** to **ham**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"class\", f.when(f.col(\"class\") == 'ham\"\"\"' , 'ham').\n",
    "                     otherwise(f.col(\"class\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets verify the change occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  ham| 4826|\n",
      "| spam|  747|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "Before we perform any analysis on the data we will need to perform two major steps:\n",
    "\n",
    "1. Text Normalization\n",
    "2. Tokenization \n",
    "\n",
    "With Text Normalization, we will process the raw text to provide a quality input for our model. These actions used the blog post [**Spam classification using Spark’s DataFrames, ML and Zeppelin (Part 1)**](https://blog.codecentric.de/en/2016/06/spam-classification-using-sparks-dataframes-ml-zeppelin-part-1/) by Daniel Pape, accessed on 4/16/2020, as guidance for some of these actions. This blog post provided a good framework particularly for handling types of text you find in an SMS message, such as emoticons.\n",
    "\n",
    "Once the raw text is normalized, we can then tokenize and convert the text into a form that can be used by the analytical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "To normalize the text, there are several steps we plan on taking:\n",
    "\n",
    "1. Convert all text to lowercase\n",
    "2. Convert all numbers to the text **_\" normalized_number \"_**\n",
    "3. Convert all emoticons to the text **_\" normalized_emoticon \"_**\n",
    "4. Convert all currency symbols to the text **_\" normalized_currency_symbol \"_**\n",
    "5. Convert all links to the text **_\" normalized_url \"_**\n",
    "6. Convert all email addresses to the text **_\" normalized_email \"_**\n",
    "7. Convert all diamond/question mark symbols to the text **_\" normalized_doamond_symbol \"_**\n",
    "8. Remove HTML characters\n",
    "9. Remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Text to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|\n",
      "+-----+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|ok lar... joking ...|\n",
      "| spam|Free entry in 2 a...|free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|u dun say so earl...|\n",
      "|  ham|Nah I don't think...|nah i don't think...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm = df.select(\"class\",\"sms\", f.lower(f.col(\"sms\")).alias(\"sms_norm\"))\n",
    "df_norm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Symbols and Objects\n",
    "To normalize the symbols and objects in the data, we will need to define user functions and employ replacement with regex tools.\n",
    "\n",
    "To enable a method to cycle through the dataframe and make all the necessary replacements, I am going to define a dictionary, where each key is the expression that will be used to find what needs to be replaces, and the value is the repalcement string.\n",
    "\n",
    "The regex for the emoticons came from [here](https://www.regextester.com/96995).\n",
    "\n",
    "The remaining regex expressions came from [here](https://github.com/daniel-pape/spark-logistic-regression-spam-sms/blob/master/src/main/scala/preprocessing/LineCleaner.scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = [\"&lt;\", \"&gt;\", \"&amp;\", \"&cent;\", \"&pound;\", \"&yen;\", \"&euro;\", \"&copy;\", \"&reg;\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_url = \"\\\\w+(\\\\.|-)*\\\\w+@.*\\\\.(com|de|uk)\"\n",
    "regex_emoticon = \":\\)|:-\\)|:\\(|:-\\(|;\\);-\\)|:-O|8-|:P|:D|:\\||:S|:\\$|:@|8o\\||\\+o\\(|\\(H\\)|\\(C\\)|\\(\\?\\)\"\n",
    "regex_number = \"\\\\d+\"\n",
    "regex_punctuation =\"[\\\\.\\\\,\\\\:\\\\-\\\\!\\\\?\\\\n\\\\t,\\\\%\\\\#\\\\*\\\\|\\\\=\\\\(\\\\)\\\\\\\"\\\\>\\\\<\\\\/]\"\n",
    "regex_currency = \"[\\\\$\\\\€\\\\£]\"\n",
    "regex_url =  \"(http://|https://)?www\\\\.\\\\w+?\\\\.(de|com|co.uk)\"\n",
    "regex_diamond_question = \"�\"\n",
    "regex_html = \"|\".join(html_list)\n",
    "\n",
    "dict_norm = {\n",
    "    regex_emoticon : \" normalized_emoticon \",\n",
    "    regex_url : \" normalized_emailadress \",\n",
    "    regex_number : \" normalized_number \",\n",
    "    regex_punctuation : \" \",\n",
    "    regex_currency : \" normalized_currency_symbol \",\n",
    "    regex_url: \" normalized_url \",\n",
    "    regex_diamond_question : \" normalized_doamond_symbol \",\n",
    "    regex_html : \" \"\n",
    "}\n",
    "\n",
    "#dict_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_norm.items():\n",
    "    df_norm = df_norm.withColumn(\"sms_norm\", f.regexp_replace(f.col(\"sms_norm\"),key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sms_norm                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|go until jurong point  crazy   available only in bugis n great world la e buffet    cine there got amore wat                                                                                                                                        |\n",
      "|ok lar    joking wif u oni                                                                                                                                                                                                                          |\n",
      "|free entry in  normalized_number  a wkly comp to win fa cup final tkts  normalized_number st may  normalized_number   text fa to  normalized_number  to receive entry question std txt rate t&c's apply  normalized_number over normalized_number 's|\n",
      "|u dun say so early hor    u c already then say                                                                                                                                                                                                      |\n",
      "|nah i don't think he goes to usf  he lives around here though                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm.select('sms_norm').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape - Rows x Columns\n",
      "5573 x 3\n"
     ]
    }
   ],
   "source": [
    "df = df_norm.dropna()\n",
    "\n",
    "print(\"Shape - Rows x Columns\")\n",
    "print(df.count(),\"x\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to check again for null values, to ensure the conversion hasn't created any new null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+\n",
      "|class|sms|sms_norm|\n",
      "+-----+---+--------+\n",
      "+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_norm\n",
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Class to Binary\n",
    "We need to convert our spam/ham class to a binary. We also need to conert the column type to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Go until jurong p...|go until jurong p...|\n",
      "|    0|Ok lar... Joking ...|ok lar    joking ...|\n",
      "|    1|Free entry in 2 a...|free entry in  no...|\n",
      "|    0|U dun say so earl...|u dun say so earl...|\n",
      "|    0|Nah I don't think...|nah i don't think...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm = df_norm.withColumn(\"class\", f.when(f.col(\"class\") == \"spam\" , 1).\n",
    "                             when(f.col(\"class\") == \"ham\" , 0).\n",
    "                             otherwise(f.col(\"class\")))\n",
    "\n",
    "df_norm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      " |-- sms_norm: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Go until jurong p...|go until jurong p...|\n",
      "|    0|Ok lar... Joking ...|ok lar    joking ...|\n",
      "|    1|Free entry in 2 a...|free entry in  no...|\n",
      "|    0|U dun say so earl...|u dun say so earl...|\n",
      "|    0|Nah I don't think...|nah i don't think...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- class: integer (nullable = true)\n",
      " |-- sms: string (nullable = true)\n",
      " |-- sms_norm: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_norm = df_norm.withColumn(\"class\", f.col('class').cast(IntegerType()))\n",
    "df_norm.show(5)\n",
    "df_norm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization\n",
    "We will tokenize the text using a pyspark pipeline. First, we must initialize the pipeline components. For this pipeline, we will user the following estimators:\n",
    "\n",
    "1. Tokenizer\n",
    "2. Stop Words Remover\n",
    "3. Term Frequency Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sms_norm\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"features_tf\", numFeatures=1000)\n",
    "\n",
    "pipeline_text = Pipeline(stages=[tokenizer, remover, hashingTF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Pipeline on Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|              tokens|     tokens_filtered|         features_tf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|Go until jurong p...|go until jurong p...|[go, until, juron...|[go, jurong, poin...|(1000,[7,77,150,1...|\n",
      "|    0|Ok lar... Joking ...|ok lar    joking ...|[ok, lar, , , , j...|[ok, lar, , , , j...|(1000,[20,372,484...|\n",
      "|    1|Free entry in 2 a...|free entry in  no...|[free, entry, in,...|[free, entry, , n...|(1000,[35,73,128,...|\n",
      "|    0|U dun say so earl...|u dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(1000,[57,368,372...|\n",
      "|    0|Nah I don't think...|nah i don't think...|[nah, i, don't, t...|[nah, think, goes...|(1000,[135,210,32...|\n",
      "|    1|FreeMsg Hey there...|freemsg hey there...|[freemsg, hey, th...|[freemsg, hey, da...|(1000,[36,91,98,9...|\n",
      "|    0|Even my brother i...|even my brother i...|[even, my, brothe...|[even, brother, l...|(1000,[18,47,48,5...|\n",
      "|    0|As per your reque...|as per your reque...|[as, per, your, r...|[per, request, 'm...|(1000,[36,71,92,1...|\n",
      "|    1|WINNER!! As a val...|winner   as a val...|[winner, , , as, ...|[winner, , , valu...|(1000,[39,43,74,1...|\n",
      "|    1|Had your mobile 1...|had your mobile  ...|[had, your, mobil...|[mobile, , normal...|(1000,[36,73,82,1...|\n",
      "|    0|I'm gonna be home...|i'm gonna be home...|[i'm, gonna, be, ...|[gonna, home, soo...|(1000,[41,159,263...|\n",
      "|    1|SIX chances to wi...|six chances to wi...|[six, chances, to...|[six, chances, wi...|(1000,[15,35,92,2...|\n",
      "|    1|URGENT! You have ...|urgent  you have ...|[urgent, , you, h...|[urgent, , won, ,...|(1000,[68,73,122,...|\n",
      "|    0|I've been searchi...|i've been searchi...|[i've, been, sear...|[searching, right...|(1000,[36,39,140,...|\n",
      "|    0|I HAVE A DATE ON ...|i have a date on ...|[i, have, a, date...|      [date, sunday]|(1000,[44,82,170,...|\n",
      "|    1|XXXMobileMovieClu...|xxxmobilemovieclu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(1000,[43,66,135,...|\n",
      "|    0|Oh k...i'm watchi...|oh k   i'm watchi...|[oh, k, , , i'm, ...|[oh, k, , , watch...|(1000,[135,275,37...|\n",
      "|    0|Eh u remember how...|eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|(1000,[14,15,80,1...|\n",
      "|    0|Fine if that��s t...|fine if that norm...|[fine, if, that, ...|[fine, normalized...|(1000,[115,129,15...|\n",
      "|    1|England v Macedon...|england v macedon...|[england, v, mace...|[england, v, mace...|(1000,[4,9,19,71,...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_text_fit = pipeline_text.fit(df_norm)\n",
    "df_pipeline = pipeline_text_fit.transform(df_norm)\n",
    "df_pipeline.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split\n",
    "Now that we have performed all the possible actions that should be performed on the complete dataset, we split the data into train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train = 0.8\n",
    "train, test = df_pipeline.randomSplit([split_train, (1-split_train)], seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape - Train\n",
      "(4423, 6)\n",
      "\n",
      "Shape - Test\n",
      "(1150, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape - Train\")\n",
    "print((train.count(), len(train.columns)))\n",
    "print()\n",
    "print(\"Shape - Test\")\n",
    "print((test.count(), len(test.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency Calculation\n",
    "To calculate the Term Frequency - Inverse Document Frequency values for the corpus, we need to train the IDF estmator on the **train** data. Then we apply the trained estimator to the train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize IDF Estimator\n",
    "idf = IDF(minDocFreq=2, inputCol=\"features_tf\", outputCol=\"features_tfidf\")\n",
    "\n",
    "# Train IDF Estimator to Term Frequency Data\n",
    "idfModel = idf.fit(train)\n",
    "\n",
    "# Re-Scale Term Frequency Data to \n",
    "train = idfModel.transform(train)\n",
    "test = idfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|              tokens|     tokens_filtered|         features_tf|      features_tfidf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0| &lt;#&gt;  in mc...|      in mca  but...|[, , , , , , in, ...|[, , , , , , mca,...|(1000,[18,83,372,...|(1000,[18,83,372,...|\n",
      "|    0| &lt;DECIMAL&gt; ...|  decimal  m but ...|[, , decimal, , m...|[, , decimal, , m...|(1000,[18,62,83,1...|(1000,[18,62,83,1...|\n",
      "|    0| and  picking the...| and  picking the...|[, and, , picking...|[, , picking, var...|(1000,[128,263,33...|(1000,[128,263,33...|\n",
      "|    0| came to look at ...| came to look at ...|[, came, to, look...|[, came, look, fl...|(1000,[7,146,197,...|(1000,[7,146,197,...|\n",
      "|    0| gonna let me kno...| gonna let me kno...|[, gonna, let, me...|[, gonna, let, kn...|(1000,[51,134,164...|(1000,[51,134,164...|\n",
      "|    0| said kiss, kiss,...| said kiss  kiss ...|[, said, kiss, , ...|[, said, kiss, , ...|(1000,[44,133,168...|(1000,[44,133,168...|\n",
      "|    0| says that he's q...| says that he's q...|[, says, that, he...|[, says, quitting...|(1000,[122,138,17...|(1000,[122,138,17...|\n",
      "|    0| what number do u...| what number do u...|[, what, number, ...|[, number, u, liv...|(1000,[281,372,49...|(1000,[281,372,49...|\n",
      "|    0|\"7 wonders in My ...|  normalized_numb...|[, , normalized_n...|[, , normalized_n...|(1000,[19,104,150...|(1000,[19,104,150...|\n",
      "|    0|\"7 wonders in My ...|  normalized_numb...|[, , normalized_n...|[, , normalized_n...|(1000,[19,104,150...|(1000,[19,104,150...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|class|                 sms|            sms_norm|              tokens|     tokens_filtered|         features_tf|      features_tfidf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0| &lt;#&gt;  mins ...|      mins but i ...|[, , , , , , mins...|[, , , , , , mins...|(1000,[73,83,183,...|(1000,[73,83,183,...|\n",
      "|    0|\"7 wonders in My ...|  normalized_numb...|[, , normalized_n...|[, , normalized_n...|(1000,[19,104,150...|(1000,[19,104,150...|\n",
      "|    0|\"A Boy loved a ga...| a boy loved a ga...|[, a, boy, loved,...|[, boy, loved, ga...|(1000,[24,29,62,6...|(1000,[24,29,62,6...|\n",
      "|    0|\"Awesome question...| awesome question...|[, awesome, quest...|[, awesome, quest...|(1000,[19,122,159...|(1000,[19,122,159...|\n",
      "|    0|\"Cool breeze... B...| cool breeze    b...|[, cool, breeze, ...|[, cool, breeze, ...|(1000,[170,196,29...|(1000,[170,196,29...|\n",
      "|    0|\"Don't Think Abou...| don't think abou...|[, don't, think, ...|[, think, \\what, ...|(1000,[7,168,237,...|(1000,[7,168,237,...|\n",
      "|    0|\"Happy or sad , o...| happy or sad   o...|[, happy, or, sad...|[, happy, sad, , ...|(1000,[44,141,168...|(1000,[44,141,168...|\n",
      "|    0|\"Storming msg: We...| storming msg  we...|[, storming, msg,...|[, storming, msg,...|(1000,[9,15,29,44...|(1000,[9,15,29,44...|\n",
      "|    0|\"Today is \\song d...| today is \\song d...|[, today, is, \\so...|[, today, \\song, ...|(1000,[19,36,83,8...|(1000,[19,36,83,8...|\n",
      "|    0|\"U need my presnt...| u need my presnt...|[, u, need, my, p...|[, u, need, presn...|(1000,[7,11,13,26...|(1000,[7,11,13,26...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate Data\n",
    "Moving forward we only need the tf-idf features and the class label. We will relabel them features and label to be consistent with the XGBoost Estmator input labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(1000,[18,83,372,...|\n",
      "|    0|(1000,[18,62,83,1...|\n",
      "|    0|(1000,[128,263,33...|\n",
      "|    0|(1000,[7,146,197,...|\n",
      "|    0|(1000,[51,134,164...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select Data\n",
    "df_train = train.select(f.col(\"class\").alias(\"label\"), f.col(\"features_tfidf\").alias(\"features\"))\n",
    "df_train.show(5)\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(1000,[73,83,183,...|\n",
      "|    0|(1000,[19,104,150...|\n",
      "|    0|(1000,[24,29,62,6...|\n",
      "|    0|(1000,[19,122,159...|\n",
      "|    0|(1000,[170,196,29...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = test.select(f.col(\"class\").alias(\"label\"), f.col(\"features_tfidf\").alias(\"features\"))\n",
    "df_test.show(5)\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "We will be training our data on the Sagemaker Pyspark XGBoost algorithm. One tricky part about using this algorithm is that it only takes **LIBSVM** format data. Unfortunatley, that is not the current format of our data. In order for the algoritm to accept our data as an input, we need to do three things\n",
    "\n",
    "1. Define the correct shema\n",
    "2. Convert data to match the correct schema\n",
    "3. Include **LibSVMRequestRowSerializer** as a parameter when initializing the XGBoost estimator.\n",
    "\n",
    "## Define the Schema\n",
    "In order to be accepted as a **LIBSVM** type data, the schema of our pyspark DataFrame must be a specific schema. The schema can be seen buried in the source code of the **Verify Schema** call in the **LibSVMRelation.scala** utility, see [here](https://github.com/apache/spark/blob/930b90a84871e2504b57ed50efa7b8bb52d3ba44/mllib/src/main/scala/org/apache/spark/ml/source/libsvm/LibSVMRelation.scala#L79) (accessed 4/17/2020). Based on this function, our data needs to be in two columns, one column a **DoubleType()** (which will be our label column) and the other column a **VectorUDT()** type (which will be our Sparse Vector features column). \n",
    "\n",
    "With these requirements, we define a general schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(label,DoubleType,true),StructField(features,VectorUDT,true)))\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType()),\n",
    "    StructField(\"features\", VectorUDT())\n",
    "])\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data to Match Schema\n",
    "We also convert the data types of our current train and test data sets to match this data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema - Train\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "\n",
      "Schema - Test\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "df_train = df_train.withColumn(\"label\", f.col('label').cast(DoubleType()))\n",
    "df_train = df_train.withColumn(\"features\", f.col('features').cast(VectorUDT()))\n",
    "print(\"Schema - Train\")\n",
    "df_train.printSchema()\n",
    "print()\n",
    "\n",
    "# Test\n",
    "df_test = df_test.withColumn(\"label\", f.col('label').cast(DoubleType()))\n",
    "df_test = df_test.withColumn(\"features\", f.col('features').cast(VectorUDT()))\n",
    "print(\"Schema - Test\")\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "We will now initialize the XGBoost Estimator. A few notes:\n",
    "\n",
    "* In order for the estimator to accept our input data as **LIBSVM**, we need to use the parameter **requestRowSerializer**. We define this parameter as **LibSVMRequestRowSerializer**, identifying the feature column, label column, and schema. \n",
    "* This is personal preference, but I like adding name to the mode we're creating. It makes it easier to find when you're looking up past trained models. So we add a **namPolicyFactory** value. But be careful. If you want to deploy your model as an endpoint, the maximum number of characters the model name can have is 63. This means the prefix you add to the front of your model can only be about 10 characters. Sagemaker will tack-on the rest of the model tag. If you exceed 63 characters, deploying your endpoint will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_estimator = XGBoostSageMakerEstimator(\n",
    "  sagemakerRole = IAMRole(role),\n",
    "  requestRowSerializer = LibSVMRequestRowSerializer(schema=schema,\n",
    "                                                    featuresColumnName=\"features\",\n",
    "                                                    labelColumnName=\"label\"),\n",
    "  trainingInstanceType = \"ml.m4.xlarge\",\n",
    "  trainingInstanceCount = 1,\n",
    "  endpointInstanceType = \"ml.m4.xlarge\",\n",
    "  endpointInitialInstanceCount = 1,\n",
    "  namePolicyFactory=RandomNamePolicyFactory(\"spam-xgb-\"),\n",
    "  endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n",
    "After initializing the model, we set the hyperparameters. This problem is a binary classification problem, so we'll et the objective to **binary:logistic** and evaluate based on the **AUC** score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_estimator.setNumRound(15)\n",
    "xgboost_estimator.setObjective(\"binary:logistic\")\n",
    "xgboost_estimator.setEvalMetric(\"auc\")\n",
    "xgboost_estimator.setSeed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "With everything set, we can now train the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost_estimator.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "## Transform\n",
    "First, we generate predictons based off the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will assign predicted labels, using 0.5 as a threshold.\n",
    "\n",
    "We will also create re-labeled columns with spam and ham. This will be primarily for downstream visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------------+-----------------+---------------+----------+\n",
      "|label|            features|      prediction|prediction_binary|prediction_spam|label_spam|\n",
      "+-----+--------------------+----------------+-----------------+---------------+----------+\n",
      "|  0.0|(1000,[73,83,183,...| 0.0203639212996|              0.0|            ham|       ham|\n",
      "|  0.0|(1000,[19,104,150...| 0.0507496446371|              0.0|            ham|       ham|\n",
      "|  0.0|(1000,[24,29,62,6...| 0.0954567715526|              0.0|            ham|       ham|\n",
      "|  0.0|(1000,[19,122,159...| 0.0118381446227|              0.0|            ham|       ham|\n",
      "|  0.0|(1000,[170,196,29...|0.00729368859902|              0.0|            ham|       ham|\n",
      "+-----+--------------------+----------------+-----------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions.withColumn(\"prediction_binary\", f.when(f.col(\"prediction\") > 0.5 , 1.0).otherwise(0.0))\n",
    "predictions = predictions.withColumn(\"prediction_spam\", f.when(f.col(\"prediction_binary\") == 1 , \"spam\").otherwise(\"ham\"))\n",
    "predictions = predictions.withColumn(\"label_spam\", f.when(f.col(\"label\") == 1 , \"spam\").otherwise(\"ham\"))\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|prediction_spam|count|\n",
      "+---------------+-----+\n",
      "|            ham| 1013|\n",
      "|           spam|  137|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(\"prediction_spam\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n",
    "Now we can look at some of the classification scores. Note that we are using both the **MulticlassClassificationEvaluator** and **BinaryClassificationEvaluator** objects to generate the metrics we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_scores(predictions):\n",
    "    ### Multi-Class Evaluator\n",
    "    dict_metric_multi = {\"Accuracy\" : \"accuracy\", \n",
    "                         \"Precision - Weighted\": \"weightedPrecision\", \n",
    "                         \"F1 Score\": \"f1\"}\n",
    "\n",
    "    for key, value in dict_metric_multi.items():\n",
    "        evaluator =  MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                                   predictionCol=\\\n",
    "                                                       \"prediction_binary\", \n",
    "                                                   metricName=value)\n",
    "\n",
    "        metric = evaluator.evaluate(predictions)\n",
    "\n",
    "        print(key + \": {:.3f}\".format(metric))   \n",
    "    \n",
    "    # Binary Class Evaluator\n",
    "    dict_metric_bin = {\"AUC Score\" : \"areaUnderROC\"}\n",
    "    for key, value in dict_metric_bin.items():\n",
    "    \n",
    "        evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", \n",
    "                                                  labelCol=\"label\", \n",
    "                                                  metricName=value)\n",
    "        \n",
    "        metric = evaluator.evaluate(predictions)\n",
    "        print(key + \": {:.3f}\".format(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980\n",
      "Precision - Weighted: 0.980\n",
      "F1 Score: 0.980\n",
      "AUC Score: 0.977\n"
     ]
    }
   ],
   "source": [
    "output_scores(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "We are going to visualize the confusion matrix using the method outlined [here](https://runawayhorse001.github.io/LearningApacheSpark/classification.html#demo).\n",
    "### Confusion Matrix Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print()\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'spam']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create List of Class Names\n",
    "class_label = predictions.select(\"label_spam\").groupBy(\"label_spam\")\\\n",
    "    .count().sort('count', ascending=False).toPandas()\n",
    "class_names = class_label[\"label_spam\"].to_list()\n",
    "class_names\n",
    "#class_names = list(map(str, class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Raw Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Labels to Pandas Dataframe\n",
    "y_true = predictions.select(\"label_spam\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "# Convert Predictions to Pandas Dataframe\n",
    "y_pred = predictions.select(\"prediction_spam\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cm, \n",
    "                      classes=class_names,\n",
    "                      normalize=True,\n",
    "                      title='Confusion Matrix, With Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-Up\n",
    "After everything is done, we do not wan't to leave resources needlessly running, costing us money. So, we shut everything down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_cleanup = SageMakerResourceCleanup(model.sagemakerClient)\n",
    "resource_cleanup.deleteResources(model.getCreatedResources())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
