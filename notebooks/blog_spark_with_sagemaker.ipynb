{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Project 3 in my **Intro to Cloud Computing** course, we were tasked with finding a problem that could be solved using a distributed platform (Spark, Hadoop / MapReduce) and then using a distributed platform to engage the problem. I chose to develop a model which identified spam text messagaes using Spark through Amazon Sagemaker and Amazon Web Services (AWS) platform. This post is going to walk through the process of developing this model and settting up all the necessary resources. To be specific is going to cover the following actions:\n",
    "\n",
    "* Estanblish a **LOCAL** Spark Session. (Future posts may connect to a Spark Cluster on Amazon EMR\n",
    "* Load text message data from Amazon S3 into Spark Session\n",
    "* Process data with a Spark ML Pipeline\n",
    " * Develop customer transformers for the ML pipeline\n",
    "* Train the resulting data on a SageMaker instance\n",
    "* Deploy the trained SageMaker model\n",
    "* Evalute the model performance by performing inference on the deployed endpoint\n",
    "\n",
    "For complete access to the project files, the associated GitHub repo for this project can be found here: \n",
    "\n",
    "[https://github.com/canfielder/DSBA-6190_Proj3_Spark_w_Sagemaker](https://github.com/canfielder/DSBA-6190_Proj3_Spark_w_Sagemaker)\n",
    "\n",
    "# Why SageMaker and Spark?\n",
    "For this project we will be performing some standard Natural Language Processing (NLP) actions on SMS messages, and doing so using a distributed platform. Now, there are many different ways to develop a distributed platform process. If we want to use a Spark cluster we can go straight to the source and use Amazon EMR or Google Cloud Dataproc. Or we could set up a MapReduce job on Azure. \n",
    "\n",
    "So why bother tapping into SageMaker?\n",
    "\n",
    "Keep in mind that a many Data Science / Machine Learning processes are going to generally follow these steps:\n",
    "\n",
    "<p align =\"middle\">\n",
    "  <img src=\"../imgs/data_science_process_flow.png\" width=\"35%\" />\n",
    "</p>\n",
    "\n",
    "Processing large quantities of text, and modeling the resulting output of that process, might have wildly different sizing needs. If you do both on the same system you would have to size that system for the most conservative usage. This means one of these steps is going to be needlessly oversized (**$$$**).\n",
    "\n",
    "Instead, we can de-couple the **Process** step from **Train** step by utilizing Spark and SageMaker togther. Instead of an oversized, single system, we can:\n",
    "\n",
    "* **Process**: Run your NLP on Amazon EMR clusters\n",
    "* **Train**: Train your model on Amazon SageMaker instances \n",
    "\n",
    "Each process is dynamically sized and spec-ed (GPUs vs no GPUs, etc.) for the actual needs of just that process, not both. In addition, even though your are tapping in to multiple Amazon resources, it is simple to develop this architecture all within a SageMaker notebook, and maybe even within the same SageMaker Pipeline.\n",
    "\n",
    "\n",
    "**Note**: I want to recognize the following Medium post, [Mixing Spark with Sagemaker ?](https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7) for providing a good brief discussion on this very subject.\n",
    "\n",
    "# Notes on Operating Environment\n",
    "I am performing all of these actions in Amazon Sagemaker on a SageMaker instance. I initially tried to perform these actions locally, connecting to Amazon resources with Boto3. But I kept getting errors, executing certain Pyspark actions, that I couldn't resolve. \n",
    "\n",
    "# Spark Cluster\n",
    "First up we will create a local SparkSession. We'll import the necessary libraries and modules, and then create the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-191-133.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sms_spam_filter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f51780e5d30>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sagemaker_pyspark\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"sms_spam_filter\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Settings and Connections\n",
    "## Boto3 Connection\n",
    "\n",
    "First things first, we're going to set up a connection to Amazon. Before we can do anything, if doing this work outside of Amazon SageMaker, make sure you have correctly set a AWS configuration file on whatever machine you are working on. See the following documentation for how to do this [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). \n",
    "\n",
    "I established a unique user and role for this exercise, which were granted the following permissions:\n",
    "\n",
    "* AmazonS3FullAccess\n",
    "* AmazonSageMakerFullAccess\n",
    "\n",
    "**Note**: *These permissions are very broad. Usually, when granting permissions, follow the principle of [Grant Least Priviledge](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "aws_credential_profile = 'blog_spam_sagemaker_spark'\n",
    "role = 'blog_spam_sagemaker_spark'\n",
    "bucket = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our region and AWS Credentials file profile defined, we can establish a connection to AWS via the SDK library Boto3. When working with Amazon SageMaker, I prefer establishing a *Session* at the start of my notebook, and then using the session to define Boto3 *Client* and *Resource* objects. This helps maintain a consistency when creating those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "boto3_session = boto3.session.Session(region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role ARN\n",
    "With a general session established, we are going to connect to the AWS IAM service and extract the ARN associated with role being used for this exercise. This ARN will be needed to access necessary resources AWS resources. The ARN could also be manually copied into this notebook, but I prefer using the Boto3 tools when available and not overly onerous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::726963482731:role/blog_spam_sagemaker_spark\n"
     ]
    }
   ],
   "source": [
    "# Establish IAM Client\n",
    "client_iam = boto3_session.client('iam')\n",
    "\n",
    "# Extract Avaiable Roles\n",
    "role_list = client_iam.list_roles()['Roles']\n",
    "\n",
    "# Initialized Role ARN variable and Establish Key Value\n",
    "role_arn = ''\n",
    "key = 'RoleName'\n",
    "\n",
    "# Extract Role ARN for Exercise Role\n",
    "for item in role_list:\n",
    "    if key in item and role == item[key]:\n",
    "        role_arn = item['Arn']\n",
    "\n",
    "# Verify ARN\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data for this project was the **SMS Spam Collection Dataset**, originally hosted on the UCI Machine Learning repository. The same dataset is also hosted on Kaggle, [here](https://www.kaggle.com/uciml/sms-spam-collection-dataset). I used the Kaggle API to load the data directly to our S3 bucket. More information on how to do that can be found on the [Kaggle GitHub page](https://github.com/Kaggle/kaggle-api) and in the main README file of my personal [GitHub page](https://github.com/canfielder/DSBA-6190_Proj3_Spark_w_Sagemaker) for this project. The result of following the necessary steps is the file *spam.csv*  in a S3 bucket, in this case named *dsba-6190-project3-spark*.\n",
    "\n",
    "With our data successfully loaded to our S3 bucket, we will now load this is data, as a **Spark DataFrame**, into the local *SparkSession* running our local notebook intance. \n",
    "\n",
    "## S3A Endpoint Check\n",
    "The code in the following block comes directly from one of the [SageMaker Spark Examples on GitHub](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-spark/pyspark_mnist/pyspark_mnist_pca_kmeans.ipynb). I have added some additional commenting for context. Essentially this code checks to see if you region is in China, an applies the correct domain suffix to the S3A endpoint to reflect this. S3A is the connector between Haddop and AWS S3. \n",
    "\n",
    "For most regions this step is unnessecary as the default settings will be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Chinese Regions\n",
    "cn_regions = ['cn-north-1', 'cn-northwest-1']\n",
    "\n",
    "# Current Region\n",
    "region = boto3_session.region_name\n",
    "\n",
    "# Defined Endpoint URL Domain Suffix (i.e.: .com)\n",
    "endpoint_domain = 'com.cn' if region in cn_regions else 'com'\n",
    "\n",
    "# Set S3A Endpoint\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.{}.amazonaws.{}'.format(region, endpoint_domain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The data we have is in CSV format. Before we load the data we need to deine a data schema. For our data, if the schema were omitted, we would be left with empyt columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define S3 Labels\n",
    "bucket = \"dsba-6190-project3-spark\"\n",
    "file_name = \"spam.csv\"\n",
    "\n",
    "# Define Known Schema\n",
    "schema = StructType([\n",
    "    StructField(\"class\", StringType()),\n",
    "    StructField(\"sms\", StringType())\n",
    "])\n",
    "\n",
    "# Import CSV\n",
    "df = spark.read\\\n",
    "          .schema(schema)\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .csv('s3a://{}/{}'.format(bucket, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "After loading the data we're going to some EDA to see what the data is we're dealing with and if there are any problems we need to sort through.\n",
    "\n",
    "First, let's take a quick look at what our data looks like by inspecting the top 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|class|sms                                                                                                                                                             |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                 |\n",
      "|ham  |Ok lar... Joking wif u oni...                                                                                                                                   |\n",
      "|spam |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's     |\n",
      "|ham  |U dun say so early hor... U c already then say...                                                                                                               |\n",
      "|ham  |Nah I don't think he goes to usf, he lives around here though                                                                                                   |\n",
      "|spam |FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv             |\n",
      "|ham  |Even my brother is not like to speak with me. They treat me like aids patent.                                                                                   |\n",
      "|ham  |As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune|\n",
      "|spam |WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.   |\n",
      "|spam |Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030      |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So pretty straight forward. Two variables. There's our target variables, **class**, and the raw text, which we will need to process before training our model.\n",
    "\n",
    "As this project is going to boil down to a binary classification exercise, let's look at the frequency counts for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| class|count|\n",
      "+------+-----+\n",
      "|ham\"\"\"|    2|\n",
      "|   ham| 4825|\n",
      "|  spam|  747|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! It appears two observations have errors in the target variable field, **ham\"\"\"**. Let's take a closer look at theses specific observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------+\n",
      "|class |sms                                                      |\n",
      "+------+---------------------------------------------------------+\n",
      "|ham\"\"\"|null                                                     |\n",
      "|ham\"\"\"|Well there's still a bit left if you guys want to tonight|\n",
      "+------+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df.where(f.col(\"class\") == 'ham\"\"\"').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, one row is null, while there other appears to be a standard **ham** message. Before doing anthing yet, let's see how many null values are in the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| class| sms|\n",
      "+------+----+\n",
      "|ham\"\"\"|null|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() \\\n",
    "                                     for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is only the one. So, We'll drop the null observation and correct the typo in the target variable for the other observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Null\n",
    "df = df.dropna()\n",
    "\n",
    "# Correct \"\"\"ham to ham\n",
    "df = df.withColumn(\"class\", f.when(f.col(\"class\") == 'ham\"\"\"' , 'ham').\n",
    "                     otherwise(f.col(\"class\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the frequency again to make sure our corrections were implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  ham| 4826|\n",
      "| spam|  747|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "Before we can model the data we need to perform three major steps:\n",
    "\n",
    "1. Text Normalization\n",
    "2. Tokenization \n",
    "3. TF-IDF Transformation\n",
    "\n",
    "With Text Normalization, we will process the raw text to provide a quality input for our model. The actions used the blog post [**Spam classification using Spark’s DataFrames, ML and Zeppelin (Part 1)**](https://blog.codecentric.de/en/2016/06/spam-classification-using-sparks-dataframes-ml-zeppelin-part-1/) by Daniel Pape, accessed on 4/16/2020, were used as guidance. This blog post provided a good framework particularly for handling types of text you find in an SMS message, such as emoticons.\n",
    "\n",
    "## Text Normalization\n",
    "To normalize the text, there are several steps we plan on taking:\n",
    "\n",
    "1. Convert all text to lowercase\n",
    "2. Convert all numbers to the text **_\" normalized_number \"_**\n",
    "3. Convert all emoticons to the text **_\" normalized_emoticon \"_**\n",
    "4. Convert all currency symbols to the text **_\" normalized_currency_symbol \"_**\n",
    "5. Convert all links to the text **_\" normalized_url \"_**\n",
    "6. Convert all email addresses to the text **_\" normalized_email \"_**\n",
    "7. Convert all diamond/question mark symbols to the text **_\" normalized_doamond_symbol \"_**\n",
    "8. Remove HTML characters\n",
    "9. Remove punctuation\n",
    "\n",
    "To better organize the regex strings and normalized text associated with each point above, I saved each regex_expression as a variable and created a dictionary cataloging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of HTML text\n",
    "html_list = [\"&lt;\", \"&gt;\", \"&amp;\", \"&cent;\", \"&pound;\", \"&yen;\", \"&euro;\", \"&copy;\", \"&reg;\"]\n",
    "\n",
    "# Regex Expressions for normalizing text\n",
    "regex_url = \"\\\\w+(\\\\.|-)*\\\\w+@.*\\\\.(com|de|uk)\"\n",
    "regex_emoticon = \":\\)|:-\\)|:\\(|:-\\(|;\\);-\\)|:-O|8-|:P|:D|:\\||:S|:\\$|:@|8o\\||\\+o\\(|\\(H\\)|\\(C\\)|\\(\\?\\)\"\n",
    "regex_number = \"\\\\d+\"\n",
    "regex_punctuation =\"[\\\\.\\\\,\\\\:\\\\-\\\\!\\\\?\\\\n\\\\t,\\\\%\\\\#\\\\*\\\\|\\\\=\\\\(\\\\)\\\\\\\"\\\\>\\\\<\\\\/]\"\n",
    "regex_currency = \"[\\\\$\\\\€\\\\£]\"\n",
    "regex_url =  \"(http://|https://)?www\\\\.\\\\w+?\\\\.(de|com|co.uk)\"\n",
    "regex_diamond_question = \"�\"\n",
    "regex_html = \"|\".join(html_list)\n",
    "\n",
    "# Dictionary of Normalized Text and Regex Expressions\n",
    "dict_norm = {\n",
    "    regex_emoticon : \" normalized_emoticon \",\n",
    "    regex_url : \" normalized_emailaddress \",\n",
    "    regex_number : \" normalized_number \",\n",
    "    regex_punctuation : \" \",\n",
    "    regex_currency : \" normalized_currency_symbol \",\n",
    "    regex_url: \" normalized_url \",\n",
    "    regex_diamond_question : \" normalized_doamond_symbol \",\n",
    "    regex_html : \" \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_col = 'sms'\n",
    "out_col = in_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../scripts/custom_transformers.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = df.withColumn(out_col, f.regexp_replace(f.col(in_col), regex_url, dict_norm[regex_url]))\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage_url = NormalizeText(inputCol=in_col, outputCol=out_col, normal_text=dict_norm[regex_url], regex_replace_string=regex_url)\n",
    "\n",
    "pipeline_text = Pipeline(stages=[stage_url])\n",
    "pipeline_text_fit = pipeline_text.fit(df)\n",
    "df_pipeline = pipeline_text_fit.transform(df)\n",
    "df_pipeline.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in  no...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile  ...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage_num = NormalizeText(inputCol=in_col, outputCol=out_col, normal_text=dict_norm[regex_number], regex_replace_string=regex_number)\n",
    "\n",
    "pipeline_text = Pipeline(stages=[stage_num])\n",
    "pipeline_text_fit = pipeline_text.fit(df)\n",
    "df_pipeline = pipeline_text_fit.transform(df)\n",
    "df_pipeline.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in  no...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile  ...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_text = Pipeline(stages=[stage_num, stage_url])\n",
    "pipeline_text_fit = pipeline_text.fit(df)\n",
    "df_pipeline = pipeline_text_fit.transform(df)\n",
    "df_pipeline.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|  ham|go until jurong p...|\n",
      "|  ham|ok lar... joking ...|\n",
      "| spam|free entry in  no...|\n",
      "|  ham|u dun say so earl...|\n",
      "|  ham|nah i don't think...|\n",
      "| spam|freemsg hey there...|\n",
      "|  ham|even my brother i...|\n",
      "|  ham|as per your reque...|\n",
      "| spam|winner!! as a val...|\n",
      "| spam|had your mobile  ...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage_lower = LowerCase(inputCol=in_col, outputCol=out_col)\n",
    "\n",
    "pipeline_text = Pipeline(stages=[stage_lower, stage_num, stage_url])\n",
    "pipeline_text_fit = pipeline_text.fit(df)\n",
    "df_pipeline = pipeline_text_fit.transform(df)\n",
    "df_pipeline.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                 sms|\n",
      "+-----+--------------------+\n",
      "|    0|Go until jurong p...|\n",
      "|    0|Ok lar... Joking ...|\n",
      "|    1|Free entry in 2 a...|\n",
      "|    0|U dun say so earl...|\n",
      "|    0|Nah I don't think...|\n",
      "|    1|FreeMsg Hey there...|\n",
      "|    0|Even my brother i...|\n",
      "|    0|As per your reque...|\n",
      "|    1|WINNER!! As a val...|\n",
      "|    1|Had your mobile 1...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage_binary = BinaryTransform(inputCol=\"class\")\n",
    "\n",
    "pipeline_text = Pipeline(stages=[stage_binary])\n",
    "pipeline_text_fit = pipeline_text.fit(df)\n",
    "df_pipeline = pipeline_text_fit.transform(df)\n",
    "df_pipeline.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                 sms|\n",
      "+-----+--------------------+\n",
      "|    0|Go until jurong p...|\n",
      "|    0|Ok lar... Joking ...|\n",
      "|    1|Free entry in 2 a...|\n",
      "|    0|U dun say so earl...|\n",
      "|    0|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_binary = df.withColumn(\"class\", f.when(f.col(\"class\") == \"spam\" , 1).\n",
    "                             when(f.col(\"class\") == \"ham\" , 0).\n",
    "                             otherwise(f.col(\"class\")))\n",
    "\n",
    "df_binary.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
