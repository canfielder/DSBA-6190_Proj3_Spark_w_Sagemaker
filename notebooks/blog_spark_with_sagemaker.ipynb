{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Project 3 in my **Intro to Cloud Computing** course, we were tasked with finding a problem that could be solved using a distributed platform (Spark, Hadoop / MapReduce) and then using a distributed platform to engage the problem. I chose to develop a model which identified spam text messagaes using Spark through Amazon Sagemaker and Amazon Web Services (AWS) platform. This post is going to walk through the process of developing this model and settting up all the necessary resources. To be specific is going to cover the following actions:\n",
    "\n",
    "* Estanblish a **LOCAL** Spark Session. (Future posts may connect to a Spark Cluster on Amazon EMR\n",
    "* Load text message data from Amazon S3 into Spark Session\n",
    "* Process data with a Spark ML Pipeline\n",
    " * Develop customer transformers for the ML pipeline\n",
    "* Train the resulting data on a SageMaker instance using SageMaker's XGBoost classification algorithm\n",
    "* Deploy the trained SageMaker model\n",
    "* Evalute the model performance by performing inference on the deployed endpoint\n",
    "\n",
    "For complete access to the project files, the associated GitHub repo for this project can be found here: \n",
    "\n",
    "[https://github.com/canfielder/DSBA-6190_Proj3_Spark_w_Sagemaker](https://github.com/canfielder/DSBA-6190_Proj3_Spark_w_Sagemaker)\n",
    "\n",
    "# Why SageMaker and Spark?\n",
    "For this project we will be performing some standard Natural Language Processing (NLP) actions on SMS messages, and doing so using a distributed platform. Now, there are many different ways to develop a distributed platform process. If we want to use a Spark cluster we can go straight to the source and use Amazon EMR or Google Cloud Dataproc. Or we could set up a MapReduce job on Azure. \n",
    "\n",
    "So why bother tapping into SageMaker?\n",
    "\n",
    "Keep in mind that a many Data Science / Machine Learning processes generally follow these overaching steps:\n",
    "\n",
    "<p align =\"middle\">\n",
    "  <img src=\"../imgs/data_science_process_flow.png\" width=\"35%\" />\n",
    "</p>\n",
    "\n",
    "We want to take special notices of steps two and three: **Process** and **Train**. Processing large quantities of text, and then training the resulting output of that process, might have wildly different sizing needs. If you do both on the same system you would have to size that system for the most conservative usage. This means one of these steps is going to be needlessly oversized (**$$$**).\n",
    "\n",
    "Instead, we can de-couple the **Process** step from **Train** step by utilizing Spark and SageMaker togther. Instead of an oversized, single system, we can:\n",
    "\n",
    "* **Process**: Run your NLP with Spark, either locally or on Amazon EMR clusters\n",
    "* **Train**: Train your model on Amazon SageMaker instances \n",
    "\n",
    "Each process is dynamically sized and spec-ed (GPUs vs no GPUs, etc.) for the actual needs of just that process, not both. In addition, even though you are tapping in to multiple Amazon resources, it is simple to develop this architecture all within a SageMaker notebook.\n",
    "\n",
    "**Note**: I want to recognize the following Medium post, [Mixing Spark with Sagemaker ?](https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7) for providing a good brief discussion on this very subject.\n",
    "\n",
    "# Notes on Operating Environment\n",
    "I am performing all of these actions in Amazon Sagemaker on a SageMaker instance. I initially tried to perform these actions locally, connecting to Amazon resources with Boto3. But I kept getting errors executing certain Pyspark actions which I couldn't resolve. Working directly in SageMaker did not result in any errors.\n",
    "\n",
    "# Spark Cluster\n",
    "First up we will create a local SparkSession. We'll import the necessary libraries and modules, and then create the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-30-124.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sms_spam_filter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa2df68cd30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sagemaker_pyspark\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"sms_spam_filter\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Settings and Connections\n",
    "## Boto3 Connection\n",
    "\n",
    "First things first, we're going to set up a connection to Amazon. Before we can do anything, if doing this work outside of Amazon SageMaker, make sure you have correctly set a AWS configuration file on whatever machine you are working on. See the following documentation for how to do this [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). \n",
    "\n",
    "I established a unique user and role for this exercise, which were granted the following permissions:\n",
    "\n",
    "* AmazonS3FullAccess\n",
    "* AmazonSageMakerFullAccess\n",
    "\n",
    "**Note**: *These permissions are very broad. Usually, when granting permissions, follow the principle of [Grant Least Priviledge](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "aws_credential_profile = 'blog_spam_sagemaker_spark'\n",
    "role = 'blog_spam_sagemaker_spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our region and AWS Credentials file profile defined, we can establish a connection to AWS via the SDK library Boto3. When working with Amazon SageMaker, I prefer establishing a *Session* at the start of my notebook, and then using the session to define Boto3 *Client* and *Resource* objects. This helps maintain a consistency when creating those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "boto3_session = boto3.session.Session(region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role ARN\n",
    "With a general session established, we are going to connect to the AWS IAM service and extract the ARN associated with role being used for this exercise. This ARN will be needed to access necessary resources AWS resources. The ARN could also be manually copied into this notebook, but I prefer using the Boto3 tools when available and not overly onerous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::726963482731:role/blog_spam_sagemaker_spark\n"
     ]
    }
   ],
   "source": [
    "# Establish IAM Client\n",
    "client_iam = boto3_session.client('iam')\n",
    "\n",
    "# Extract Avaiable Roles\n",
    "role_list = client_iam.list_roles()['Roles']\n",
    "\n",
    "# Initialized Role ARN variable and Establish Key Value\n",
    "role_arn = ''\n",
    "key = 'RoleName'\n",
    "\n",
    "# Extract Role ARN for Exercise Role\n",
    "for item in role_list:\n",
    "    if key in item and role == item[key]:\n",
    "        role_arn = item['Arn']\n",
    "\n",
    "# Verify ARN\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n",
    "The data for this project was the **SMS Spam Collection Dataset**, originally hosted on the UCI Machine Learning repository. The same dataset is also hosted on Kaggle, [here](https://www.kaggle.com/uciml/sms-spam-collection-dataset). I used the Kaggle API to load the data directly to our S3 bucket. More information on how to do that can be found on the [Kaggle GitHub page](https://github.com/Kaggle/kaggle-api) and in the main README file of my personal [GitHub page](https://github.com/canfielder/DSBA-6190_Proj3_Spark_w_Sagemaker) for this project. The result of following the necessary steps is the file *spam.csv*  in a S3 bucket, in this case named *dsba-6190-project3-spark*.\n",
    "\n",
    "With our data successfully loaded to our S3 bucket, we will now load this is data, as a **Spark DataFrame**, into the local *SparkSession* running our local notebook intance. \n",
    "\n",
    "## S3A Endpoint Check\n",
    "The code in the following block comes directly from one of the [SageMaker Spark Examples on GitHub](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-spark/pyspark_mnist/pyspark_mnist_pca_kmeans.ipynb). I have added some additional commenting for context. Essentially this code checks to see if you region is in China, an applies the correct domain suffix to the S3A endpoint to reflect this. S3A is the connector between Haddop and AWS S3. \n",
    "\n",
    "For most regions this step is unnessecary as the default settings will be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Chinese Regions\n",
    "cn_regions = ['cn-north-1', 'cn-northwest-1']\n",
    "\n",
    "# Current Region\n",
    "region = boto3_session.region_name\n",
    "\n",
    "# Defined Endpoint URL Domain Suffix (i.e.: .com)\n",
    "endpoint_domain = 'com.cn' if region in cn_regions else 'com'\n",
    "\n",
    "# Set S3A Endpoint\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.{}.amazonaws.{}'.format(region, endpoint_domain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The data we have is in CSV format. We're going to import this CSV into a pyspark DataFrame. But, before we load the data we need to define a schema. If a schema was not pre-defined, this particular dataset would have been imported with empty columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define S3 Labels\n",
    "bucket = \"dsba-6190-project3-spark\"\n",
    "file_name = \"spam.csv\"\n",
    "\n",
    "# Define Known Schema\n",
    "schema = StructType([\n",
    "    StructField(\"class\", StringType()),\n",
    "    StructField(\"sms\", StringType())\n",
    "])\n",
    "\n",
    "# Import CSV\n",
    "df = spark.read\\\n",
    "          .schema(schema)\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .csv('s3a://{}/{}'.format(bucket, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Data\n",
    "After loading the data we're going to inspect the data to see what we're dealing with, and if there are any problems we need to sort through.\n",
    "\n",
    "First, let's take a quick look at what our data looks like by inspecting the top 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|class|sms                                                                                                                                                             |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                 |\n",
      "|ham  |Ok lar... Joking wif u oni...                                                                                                                                   |\n",
      "|spam |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's     |\n",
      "|ham  |U dun say so early hor... U c already then say...                                                                                                               |\n",
      "|ham  |Nah I don't think he goes to usf, he lives around here though                                                                                                   |\n",
      "|spam |FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv             |\n",
      "|ham  |Even my brother is not like to speak with me. They treat me like aids patent.                                                                                   |\n",
      "|ham  |As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune|\n",
      "|spam |WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.   |\n",
      "|spam |Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030      |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So pretty straight forward. Two variables. There's our target variables, **class**, and the raw text, which we will need to process before training our model.\n",
    "\n",
    "As this project is going to boil down to a binary classification exercise, let's look at the frequency counts for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| class|count|\n",
      "+------+-----+\n",
      "|ham\"\"\"|    2|\n",
      "|   ham| 4825|\n",
      "|  spam|  747|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! It appears two observations have errors in the target variable field, **ham\"\"\"**. Let's take a closer look at theses specific observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------+\n",
      "|class |sms                                                      |\n",
      "+------+---------------------------------------------------------+\n",
      "|ham\"\"\"|null                                                     |\n",
      "|ham\"\"\"|Well there's still a bit left if you guys want to tonight|\n",
      "+------+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df.where(f.col(\"class\") == 'ham\"\"\"').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, one row is null, while there other appears to be a standard **ham** message. Before doing anthing yet, let's see how many null values are in the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| class| sms|\n",
      "+------+----+\n",
      "|ham\"\"\"|null|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() \\\n",
    "                                     for x in df.columns))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is only the one. So, We'll drop the null observation and correct the typo in the target variable for the other observation. Then we'll check the target variable frequency again to make sure our corrections were implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  ham| 4826|\n",
      "| spam|  747|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Null\n",
    "df = df.dropna()\n",
    "\n",
    "# Correct \"\"\"ham to ham\n",
    "df = df.withColumn(\"class\", f.when(f.col(\"class\") == 'ham\"\"\"' , 'ham').\n",
    "                     otherwise(f.col(\"class\")))\n",
    "\n",
    "# Generate Target Frequency Count\n",
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Machine Learning Pipeline\n",
    "\n",
    "In order to fully take advantage of using Spark and SageMaker, we're going to want to create a Spark ML Pipeline that will perform all of the actions for both the **Process** and **Train** steps. We can see the documentation on pipelines here: [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html).\n",
    "\n",
    "There are three main components for our ML Pipeline:\n",
    "\n",
    "* DataFrame\n",
    "* Transformer\n",
    "* Estimator\n",
    "\n",
    "Our DataFrame has already been loaded in the previous steps.\n",
    "\n",
    "Our Transformers are going to handle processing of the DataFrame. These are Spark components.\n",
    "\n",
    "Our Estimator is going to model the final, processed data. This is a SageMaker component. \n",
    "\n",
    "## Process\n",
    "Before the our data can reach the modeling portion of our Pipeline, it needs to pass through the processing portion. To process our data we are either going to use a pre-existing Transformers, or define our own custom Transformers. These transformers will need to cover the following major steps.\n",
    "\n",
    "1. Convert Target Variable to binary double-type \n",
    "2. Text Normalization\n",
    "3. Tokenization \n",
    "4. TF-IDF Transformation\n",
    "\n",
    "For custom variables, I recommend creating a separate python script for defining these Transformers, and then loading this script into our notebook when needed. The customer Transformers are defined in the file *custom_transformers.py*, located in a directy named *scripts*, found in the working directory of this project.\n",
    "\n",
    "### Final Data Format\n",
    "At the end of the Process portion of the Pipeline, we need our DataFrame to be two columns. We need a column of Doubles named **label** (our tagret variable), and a column of Vectors of Doubles name **features** (our processed text). This is the format our SageMaker XGBoost algorithm will understand.\n",
    "\n",
    "### Target Variable \n",
    "Currently our target variable is a string-type value labeled either spam or ham. We need to convert this to an double-type value of 1 and 0. For this we defined a custom Transformer, shown below. \n",
    "\n",
    "#### Custom Transformer: Binary Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Custom Transformer Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "With Text Normalization, we will process the raw text to provide a quality input for our model. The actions used the blog post [**Spam classification using Spark’s DataFrames, ML and Zeppelin (Part 1)**](https://blog.codecentric.de/en/2016/06/spam-classification-using-sparks-dataframes-ml-zeppelin-part-1/) by Daniel Pape, accessed on 4/16/2020, were used as guidance. This blog post provided a good framework particularly for handling types of text you find in an SMS message, such as emoticons.\n",
    "\n",
    "To normalize the text, there are several steps we plan on taking:\n",
    "\n",
    "1. Convert all text to lowercase\n",
    "2. Convert all numbers to the text **_\" normalized_number \"_**\n",
    "3. Convert all emoticons to the text **_\" normalized_emoticon \"_**\n",
    "4. Convert all currency symbols to the text **_\" normalized_currency_symbol \"_**\n",
    "5. Convert all links to the text **_\" normalized_url \"_**\n",
    "6. Convert all email addresses to the text **_\" normalized_email \"_**\n",
    "7. Convert all diamond/question mark symbols to the text **_\" normalized_doamond_symbol \"_**\n",
    "8. Remove HTML characters\n",
    "9. Remove punctuation\n",
    "\n",
    "These 9 steps can be executed using two custom Transformers. The first will convert all text to lower case. \n",
    "#### Custom Transformer: Convert Text to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Custom Transformer Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second custom Transformer will execute steps two through nine. In addition to taking in an input and output column, it also takes a regex expression and a string. This Transformer searches for the regex expression and replaces anything that matches with a string. So, we will call this custom Transformer 8 times, redefining the regex expression and replacement string for the appropriate text normalization.\n",
    "\n",
    "#### Custom Transformer: Replace Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Custom Transformer Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better organize the regex strings and normalized text associated with each point above, I saved each regex_expression as a variable and created a dictionary cataloging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of HTML text\n",
    "html_list = [\"&lt;\", \"&gt;\", \"&amp;\", \"&cent;\", \"&pound;\", \"&yen;\", \"&euro;\", \"&copy;\", \"&reg;\"]\n",
    "\n",
    "# Regex Expressions for normalizing text\n",
    "regex_email = \"\\\\w+(\\\\.|-)*\\\\w+@.*\\\\.(com|de|uk)\"\n",
    "regex_emoticon = \":\\)|:-\\)|:\\(|:-\\(|;\\);-\\)|:-O|8-|:P|:D|:\\||:S|:\\$|:@|8o\\||\\+o\\(|\\(H\\)|\\(C\\)|\\(\\?\\)\"\n",
    "regex_number = \"\\\\d+\"\n",
    "regex_punctuation =\"[\\\\.\\\\,\\\\:\\\\-\\\\!\\\\?\\\\n\\\\t,\\\\%\\\\#\\\\*\\\\|\\\\=\\\\(\\\\)\\\\\\\"\\\\>\\\\<\\\\/]\"\n",
    "regex_currency = \"[\\\\$\\\\€\\\\£]\"\n",
    "regex_url =  \"(http://|https://)?www\\\\.\\\\w+?\\\\.(de|com|co.uk)\"\n",
    "regex_diamond_question = \"�\"\n",
    "regex_html = \"|\".join(html_list)\n",
    "\n",
    "# Dictionary of Normalized Text and Regex Expressions\n",
    "dict_norm = {\n",
    "    regex_emoticon : \" normalized_emoticon \",\n",
    "    regex_email : \" normalized_emailaddress \",\n",
    "    regex_number : \" normalized_number \",\n",
    "    regex_punctuation : \" \",\n",
    "    regex_currency : \" normalized_currency_symbol \",\n",
    "    regex_url: \" normalized_url \",\n",
    "    regex_diamond_question : \" normalized_doamond_symbol \",\n",
    "    regex_html : \" \"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create one more custom Transformer that selects the **label** and **features** column from our processed data. \n",
    "\n",
    "#### Custom Transformer: Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Custom Transformer Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PipeLine Stages\n",
    "Now that we have gone over each Transformer, we need to initialize them before incorporating them into the pipeline. But first, we need to run our script of custom Transformers, so that our notebook recognizes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../scripts/custom_transformers.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our custom Transformers loaded, we can initialize all of our custom processing Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = 'features'\n",
    "\n",
    "stage_binary_target=BinaryTransform(inputCol=\"class\")\n",
    "\n",
    "stage_lowercase_text=LowerCase(inputCol= \"sms\", outputCol=output_col)\n",
    "\n",
    "stage_norm_emot=NormalizeText(inputCol=stage_lowercase_text.getOutputCol(), \n",
    "                               outputCol=output_col, \n",
    "                               normal_text=dict_norm[regex_emoticon],\n",
    "                               regex_replace_string=regex_emoticon)\n",
    "\n",
    "stage_norm_email=NormalizeText(inputCol=stage_norm_emot.getOutputCol(), \n",
    "                               outputCol=output_col, \n",
    "                               normal_text=dict_norm[regex_email],\n",
    "                               regex_replace_string=regex_email)\n",
    "\n",
    "stage_norm_num=NormalizeText(inputCol=stage_norm_email.getOutputCol(), \n",
    "                               outputCol=output_col, \n",
    "                               normal_text=dict_norm[regex_number],\n",
    "                               regex_replace_string=regex_number)\n",
    "\n",
    "stage_norm_punct=NormalizeText(inputCol=stage_norm_num.getOutputCol(), \n",
    "                               outputCol=output_col, \n",
    "                               normal_text=dict_norm[regex_punctuation],\n",
    "                               regex_replace_string=regex_punctuation)\n",
    "\n",
    "stage_norm_cur=NormalizeText(inputCol=stage_norm_punct.getOutputCol(), \n",
    "                               outputCol=output_col, \n",
    "                               normal_text=dict_norm[regex_currency],\n",
    "                               regex_replace_string=regex_currency)\n",
    "\n",
    "stage_norm_url=NormalizeText(inputCol=stage_norm_cur.getOutputCol(), \n",
    "                               outputCol=output_col, \n",
    "                               normal_text=dict_norm[regex_url],\n",
    "                               regex_replace_string=regex_url)\n",
    "\n",
    "stage_norm_diamond=NormalizeText(inputCol=stage_norm_url.getOutputCol(), \n",
    "                               outputCol=output_col, \n",
    "                               normal_text=dict_norm[regex_diamond_question],\n",
    "                               regex_replace_string=regex_diamond_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize our pre-definied Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Bag of Words Tokens\n",
    "tokens = Tokenizer(inputCol=output_col, outputCol='tokens')\n",
    "\n",
    "# Remove Stopwords\n",
    "stop_words = StopWordsRemover(inputCol=tokens.getOutputCol(), \n",
    "                           outputCol=\"tokens_filtered\")\n",
    "\n",
    "# Term Frequency Hash\n",
    "hashingTF = HashingTF(inputCol=stop_words.getOutputCol(),\n",
    "                      outputCol=\"tokens_hashed\", numFeatures=1000)\n",
    "\n",
    "# IDF \n",
    "idf = IDF(minDocFreq=2, inputCol=hashingTF.getOutputCol(), outputCol=\"features_tfidf\")\n",
    "\n",
    "# Final Processing Step - Custom\n",
    "stage_column_select=ColumnSelect(inputCol=idf.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline=Pipeline(stages=[stage_binary_target, stage_lowercase_text, \n",
    "                               stage_norm_emot, stage_norm_url,\n",
    "                               stage_norm_num, stage_norm_punct,\n",
    "                               stage_norm_cur, stage_norm_url,\n",
    "                               stage_norm_diamond, tokens, stop_words,\n",
    "                               hashingTF, idf, stage_column_select]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(1000,[7,77,150,1...|\n",
      "|  0.0|(1000,[20,372,484...|\n",
      "|  1.0|(1000,[35,73,128,...|\n",
      "|  0.0|(1000,[57,372,526...|\n",
      "|  0.0|(1000,[210,340,37...|\n",
      "|  1.0|(1000,[91,99,120,...|\n",
      "|  0.0|(1000,[47,57,330,...|\n",
      "|  0.0|(1000,[71,92,192,...|\n",
      "|  1.0|(1000,[39,43,74,1...|\n",
      "|  1.0|(1000,[73,146,224...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_fit = pipeline.fit(df)\n",
    "df_pipeline = pipeline_fit.transform(df)\n",
    "df_pipeline.show(10)\n",
    "df_pipeline.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
